\chapter{ML Models}

In this thesis, I have implemented and tested 3 various models:
\begin{enumerate}
    \item Baseline - Random forest classifier
    \item Random forest classifier on surroundings
    \item REFINED: https://doi.org/10.1038/s41598-021-90923-y
    
\end{enumerate}

\section{Model interface}

All of these models have a common interface of a method  \texttt{predict(protein)}, which takes a protein as an input and returns the predicted probability of each input point being a ligand binding site.

\subsection{Input protein}

The input protein is a 3D cloud of points with features, where each point represents an atom [TK: TRUE?] in the protein and features characterize chemical and biological characteristics of the described area.

In the program, this is represented by a 2D array of features by locations. The physical position is one of the features, using which we can recreate the points cloud.

\subsection{Output probabilities}

The output of the models is the predicted probability of each location to be a ligand binding site. Using the input order, the output is simply an array of predicted probabilities.

\section{Baseline Random Forest Model}

As a baseline model, we are using a random forest classifier as described in the original paper. It does training and prediction point-wise.

This means that it predicts the probability of a site being ligand binding by only the features logged for the given site.


\section{Random Forest Model on surroundings}

This model uses RFC as well, but it uses not only the given site's features but also features of the surrounding \textit{k} sites, extracted as described above[TK, link].

\section{REFINED}
% https://www.nature.com/articles/s41467-020-18197-y
This model follows the following steps

\begin{enumerate}
    \item Input: Set of samples $X = x_1, ... x_n$ in form of matrices of size \textit{single point features} x \textit{surroundings size} = N.
    \item We regulize each feature to follow $f \sim N(0,1)$
    \item We minimize over all permutations of features the function of 
    $$ f(X) = \sum_{s=1}^{n} \sum_{i, j, k, l =1}^n D(x_{s, ij}, x_{s, kl}) d((i,j),(k,l)^{-1}$$
    Where $d$ is the euclidian distance and $D$ absolute difference.\\
    As this is computationally exponential, we approximate this using the hill climbing algorithm.\\
    Tk: In the original paper this is done with basian noise and a lot of continuous math - but in principle the same.
    I don't know why. Maybe because it allows them to run this on GPU for some reason? Or it help the HCA?
    \item Now we train a CNN-based classifier on the created dataset. Tk: Explain CNNs
    
\end{enumerate}

%  https://www.nature.com/articles/s41598-021-90923-y#citeas

