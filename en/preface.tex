\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Most common \textit{machine learning} (ML) models take a feature vector as an input and, from it, predict some value. In this work, I explore a new approach in which a feature vector is first transformed into an image, which is then used as input into a Convolutional Neural Network (CNN).

This work aims to explore whether the results of this approach are better than those of commonly used methods, such as random forests or simple dense neural networks.

For this approach to make sense, it requires high-dimensional data (an image with a resolution of 3x3 values doesn't make much sense). Because of this, I test this approach on data generated by \cite{P2RANK}, which is in the form of a point cloud. The feature vector can be extended by appending the features from the N nearest points. This allows us to create datasets with an arbitrary amount of features.

Part of the \cite{P2RANK} is also training a ranking model, which allows us to set a baseline for the models.

The simplest way to get an image from a vector is to reshape the vector into a matrix (a black-and-white image in practice). However, this simplest way could be improved by rearranging the pixels into a more precisely defined order.

I explore a variation of the REFINED (\cite{REFINED}) algorithm, which achieves gradients in the image by minimizing the difference between adjacent pixels. This approach is visualized in Figure [tk. source]. Then I compare it to other state-of-the-art approaches and discuss whether visualizing the data with it makes any sense for human evaluation. I compare the model performance in precision, recall, and f1 scores and contrast it with the needed time for training and inference. 